import requests
import json
from datetime import datetime
import re

HN_API_BASE = "https://hacker-news.firebaseio.com/v0"

AI_KEYWORDS = {
            'AI', 'artificial intelligence', 'machine learning', 'ML', 'LLM',
            'Claude', 'ChatGPT', 'GPT', 'Gemini', 'deep learning', 'transformer',
            'neural', 'LLM', 'language model', 'foundation model'
}

def get_top_stories():
            url = f"{HN_API_BASE}/topstories.json"
            response = requests.get(url, timeout=10)
            return response.json()[:100]

def get_item(item_id):
            url = f"{HN_API_BASE}/item/{item_id}.json"
            try:
                            response = requests.get(url, timeout=5)
                            return response.json()
                        except:
        return None

def extract_summary(text):
            if not text:
                            return "æ— æ‘˜è¦"
                        text = re.sub('<[^<]+?>', '', text)
    text = text.strip()
    if len(text) > 150:
                    return text[:150] + "..."
                return text

def is_ai_related(title, text=""):
            content = (title + " " + text).lower()
    match_count = sum(1 for keyword in AI_KEYWORDS if keyword.lower() in content)
    return match_count >= 1

def fetch_ai_news():
            ai_stories = []
    print("è·å–æœ€æ–°æ•…äº‹...")
    story_ids = get_top_stories()

    for idx, story_id in enumerate(story_ids):
                    item = get_item(story_id)
                    if not item:
                                        continue
                                    if item.get('deleted') or item.get('dead'):
                                                        continue

        title = item.get('title', '')
        text = item.get('text', '')
        url = item.get('url', '')

        if is_ai_related(title, text):
                            summary = extract_summary(text) if text else "æ— æ‘˜è¦"

            ai_stories.append({
                                    'id': item['id'],
                                    'title': title,
                                    'url': url,
                                    'score': item.get('score', 0),
                                    'by': item.get('by', 'Unknown'),
                                    'comments': item.get('descendants', 0),
                                    'summary': summary
            })

        if (idx + 1) % 20 == 0:
                            print(f"å·²æ£€æŸ¥{idx+1}ä¸ªæ•…äº‹ï¼Œæ‰¾åˆ°{len(ai_stories)}ç¯‡...")

    return ai_stories

def format_markdown(stories):
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    content = f"# ğŸ¤– Hacker News AIç›¸å…³å†…å®¹\n\n**æ›´æ–°**: {timestamp}\n**æ‰¾åˆ°**: {len(stories)} ç¯‡\n\n---\n\n"

    stories = sorted(stories, key=lambda x: x['score'], reverse=True)

    for idx, story in enumerate(stories, 1):
                    content += f"## {idx}. {story['title']}\n\n"
        content += f"**è¯„åˆ†**: {story['score']} | **è¯„è®º**: {story['comments']}\n"
        content += f"**ä½œè€…**: {story['by']}\n\n"
        content += f"**æ‘˜è¦**: {story['summary']}\n\n"
        if story['url']:
                            content += f"**é“¾æ¥**: {story['url']}\n\n"
                        content += "---\n\n"

    return content

def main():
            print("å¼€å§‹è·å–Hacker News AIç›¸å…³å†…å®¹...")
    ai_stories = fetch_ai_news()
    print(f"æ‰¾åˆ°{len(ai_stories)}ç¯‡AIç›¸å…³å†…å®¹")

    markdown = format_markdown(ai_stories)

    with open('AI_NEWS.md', 'w', encoding='utf-8') as f:
                    f.write(markdown)
    print("å·²ä¿å­˜åˆ°AI_NEWS.md")

    with open('ai_news.json', 'w', encoding='utf-8') as f:
                    json.dump(ai_stories, f, ensure_ascii=False, indent=2)
    print("å·²ä¿å­˜åˆ°ai_news.json")

if __name__ == "__main__":
            main()
