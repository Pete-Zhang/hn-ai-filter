import requests
import json
from datetime import datetime
import re
from html.parser import HTMLParser

# Hacker News APIåŸºç¡€URLï¼ˆå…è´¹ï¼Œæ— éœ€è®¤è¯ï¼‰
HN_API_BASE = "https://hacker-news.firebaseio.com/v0"

# æ ¸å¿ƒAIå…³é”®è¯ - é«˜åº¦ç›¸å…³
CORE_AI_KEYWORDS = [
        'AI', 'artificial intelligence', 'machine learning', 'ML', 'LLM', 
        'Claude', 'ChatGPT', 'GPT', 'Gemini', 'deep learning', 'transformer',
        'neural network', 'language model', 'foundation model', 'agent',
        'autonomous system', 'RAG', 'prompt', 'fine-tune', 'training'
]

# ç›¸å…³AIå…³é”®è¯ - è¾…åŠ©åˆ¤æ–­
RELATED_AI_KEYWORDS = [
        'data science', 'algorithm', 'neural', 'model', 'prediction',
        'classification', 'regression', 'NLP', 'vision', 'computer vision'
]

def get_top_stories():
        """è·å–æœ€æ–°çš„æ•…äº‹IDåˆ—è¡¨"""
        url = f"{HN_API_BASE}/topstories.json"
        response = requests.get(url, timeout=10)
        return response.json()[:150]  # è·å–å‰150ä¸ªç¡®ä¿è¶³å¤Ÿæ•°é‡

def get_item(item_id):
        """è·å–å•ä¸ªæ•…äº‹çš„è¯¦ç»†ä¿¡æ¯"""
        url = f"{HN_API_BASE}/item/{item_id}.json"
        try:
                    response = requests.get(url, timeout=5)
                    return response.json()
                except:
        return None

def strip_html_tags(text):
        """ç§»é™¤HTMLæ ‡ç­¾"""
        if not text:
                    return ""
                # ç®€å•çš„HTMLæ ‡ç­¾ç§»é™¤
    text = re.sub('<[^<]+?>', '', text)
    # ç§»é™¤å¤šä¸ªç©ºæ ¼
    text = re.sub('\s+', ' ', text)
    return text.strip()

def extract_summary(text, max_length=200):
        """ä»æ–‡æœ¬ä¸­æå–æ‘˜è¦ï¼ˆå‰å‡ å¥è¯ï¼‰"""
        if not text:
                    return "æ— æ‘˜è¦"

    text = strip_html_tags(text)

    # æŒ‰å¥å­åˆ†å‰²ï¼ˆç®€å•æ–¹æ³•ï¼‰
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', text)

    summary = ""
    for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 5:  # é¿å…å¤ªçŸ­çš„å¥å­
                                if len(summary) + len(sentence) <= max_length:
                                                    summary += sentence + "ã€‚"
                else:
                                    break

    if not summary:
                # å¦‚æœæ²¡æœ‰å¥å­ï¼Œç›´æ¥æˆªæ–­
                summary = text[:max_length] + "..." if len(text) > max_length else text

    return summary if summary else "æ— æ‘˜è¦"

def get_article_content(url):
        """å°è¯•è·å–æ–‡ç« å†…å®¹ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        if not url or url.startswith('item?'):
                    return None

    try:
                response = requests.get(url, timeout=5)
                # ç®€å•çš„å†…å®¹æå–
        text = response.text
        # ç§»é™¤HTMLæ ‡ç­¾å¹¶æå–ä¸»è¦å†…å®¹
        text = re.sub('<script[^<]*</script>', '', text, flags=re.DOTALL)
        text = re.sub('<style[^<]*</style>', '', text, flags=re.DOTALL)
        text = strip_html_tags(text)

        # å–å‰500å­—ä½œä¸ºå†…å®¹
        return text[:500]
    except:
        return None

def is_highly_ai_related(title, text=""):
        """ç²¾å‡†åˆ¤æ–­å†…å®¹æ˜¯å¦ä¸AIé«˜åº¦ç›¸å…³"""
        content = (title + " " + text).lower()

    # è®¡ç®—æ ¸å¿ƒå…³é”®è¯åŒ¹é…æ•°
    core_matches = sum(1 for keyword in CORE_AI_KEYWORDS if keyword.lower() in content)

    # è®¡ç®—ç›¸å…³å…³é”®è¯åŒ¹é…æ•°
    related_matches = sum(1 for keyword in RELATED_AI_KEYWORDS if keyword.lower() in content)

    # æ’é™¤éAIç›¸å…³çš„å‡é˜³æ€§è¯æ±‡
    exclusions = [
                'mushroom', 'hallucination', 'parasite', 'spain', 'gold', 
                'gaming', 'game', 'rainbow six', 'video game',
                'history', 'ancient', 'public domain'
    ]

    for exclusion in exclusions:
                if exclusion.lower() in content:
                                return False

    # åˆ¤æ–­é€»è¾‘ï¼šéœ€è¦è‡³å°‘1ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼Œæˆ–2ä¸ªç›¸å…³å…³é”®è¯
    is_related = core_matches >= 1 or related_matches >= 2

    return is_related

def fetch_ai_news():
        """è·å–æ‰€æœ‰AIç›¸å…³çš„æ–°é—»"""
        ai_stories = []

    print("ğŸ” æ­£åœ¨è·å–æœ€æ–°æ•…äº‹...")
    story_ids = get_top_stories()

    for idx, story_id in enumerate(story_ids):
                item = get_item(story_id)
                if not item:
                                continue

        # è·³è¿‡å·²åˆ é™¤çš„é¡¹ç›®
        if item.get('deleted') or item.get('dead'):
                        continue

        title = item.get('title', '')
        text = item.get('text', '')
        url = item.get('url', '')

        # æ£€æŸ¥æ˜¯å¦ä¸AIé«˜åº¦ç›¸å…³
        if is_highly_ai_related(title, text):
                        # å°è¯•è·å–æ–‡ç« å†…å®¹ä½œä¸ºæ‘˜è¦
                        summary = None
                        if url:
                                            content = get_article_content(url)
                                            if content:
                                                                    summary = extract_summary(content)

            # å¦‚æœæ²¡æœ‰è·å–åˆ°ï¼Œå°è¯•ä½¿ç”¨æ–‡ç« ä¸­çš„textå­—æ®µ
            if not summary:
                                if text:
                                                        summary = extract_summary(text)
            else:
                    summary = f"æš‚æ— æ‘˜è¦ - {title[:100]}"

            ai_stories.append({
                                'id': item['id'],
                                'title': title,
                                'url': url,
                                'score': item.get('score', 0),
                                'by': item.get('by', 'Unknown'),
                                'time': item.get('time', 0),
                                'comments': item.get('descendants', 0),
                                'summary': summary,
                                'hn_url': f"https://news.ycombinator.com/item?id={item['id']}"
            })

        # æ˜¾ç¤ºè¿›åº¦
        if (idx + 1) % 20 == 0:
                        print(f"  å·²æ£€æŸ¥ {idx + 1}/{len(story_ids)} ä¸ªæ•…äº‹ï¼Œå·²æ‰¾åˆ° {len(ai_stories)} ç¯‡AIç›¸å…³å†…å®¹...")

    return ai_stories

def format_results(ai_stories):
        """æ ¼å¼åŒ–ç»“æœä¸ºMarkdown"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    content = f"""# ğŸ¤– Hacker News AIç›¸å…³å†…å®¹ç­›é€‰ç»“æœ

    **æ›´æ–°æ—¶é—´**: {timestamp}  
    **æ‰¾åˆ°**: {len(ai_stories)} ç¯‡é«˜åº¦ç›¸å…³AIå†…å®¹

    ---

    """

    # æŒ‰åˆ†æ•°æ’åº
    ai_stories.sort(key=lambda x: x['score'], reverse=True)

    for idx, story in enumerate(ai_stories, 1):
                content += f"""## {idx}. {story['title']}

                **æ¥æº**: [Hacker News](https://news.ycombinator.com/item?id={story['id']})  
                **è¯„åˆ†**: {story['score']} ğŸ‘ | **è¯„è®º**: {story['comments']} ğŸ’¬  
                **ä½œè€…**: {story['by']}

                **æ‘˜è¦**: {story['summary']}

                """

        if story['url']:
                        content += f"**åŸæ–‡é“¾æ¥**: [{story['url'][:60]}...]({story['url']})\n\n"

        content += "---\n\n"

    return content

def main():
        print("ğŸš€ å¼€å§‹è·å–Hacker News AIç›¸å…³å†…å®¹...")

    # è·å–AIç›¸å…³çš„æ–°é—»
    ai_stories = fetch_ai_news()

    print(f"\nâœ… æ‰¾åˆ° {len(ai_stories)} ç¯‡é«˜åº¦ç›¸å…³çš„AIå†…å®¹")

    # æ ¼å¼åŒ–ä¸ºMarkdown
    markdown_content = format_results(ai_stories)

    # ä¿å­˜åˆ°æ–‡ä»¶
    with open('AI_NEWS.md', 'w', encoding='utf-8') as f:
                f.write(markdown_content)

    print("ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° AI_NEWS.md")

    # åŒæ—¶ä¿å­˜JSONæ ¼å¼ï¼ˆç”¨äºå…¶ä»–å¤„ç†ï¼‰
    with open('ai_news.json', 'w', encoding='utf-8') as f:
                json.dump(ai_stories, f, ensure_ascii=False, indent=2)

    print("ğŸ“Š JSONæ•°æ®å·²ä¿å­˜åˆ° ai_news.json")

if __name__ == "__main__":
        main()
