import requests
import json
from datetime import datetime
import re

# Hacker News APIåŸºç¡€URLï¼ˆå…è´¹ï¼Œæ— éœ€è®¤è¯ï¼‰
HN_API_BASE = "https://hacker-news.firebaseio.com/v0"

# AIç›¸å…³å…³é”®è¯ï¼ˆå¯æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
AI_KEYWORDS = [
    'AI', 'artificial intelligence', 'machine learning', 'ML', 'LLM', 'Claude',
    'ChatGPT', 'GPT', 'Gemini', 'neural', 'deep learning', 'transformer',
    'agent', 'autonomous', 'language model', 'foundation model', 'RAG'
]

def get_top_stories():
    """è·å–æœ€æ–°çš„æ•…äº‹IDåˆ—è¡¨"""
    url = f"{HN_API_BASE}/topstories.json"
    response = requests.get(url, timeout=10)
    return response.json()[:100]  # è·å–å‰100ä¸ª

def get_item(item_id):
    """è·å–å•ä¸ªæ•…äº‹çš„è¯¦ç»†ä¿¡æ¯"""
    url = f"{HN_API_BASE}/item/{item_id}.json"
    try:
        response = requests.get(url, timeout=5)
        return response.json()
    except:
        return None

def is_ai_related(title, text=""):
    """åˆ¤æ–­å†…å®¹æ˜¯å¦ä¸AIé«˜åº¦ç›¸å…³"""
    content = (title + " " + text).lower()
    
    # è®¡ç®—åŒ¹é…çš„å…³é”®è¯æ•°é‡
    matched_keywords = sum(1 for keyword in AI_KEYWORDS if keyword.lower() in content)
    
    # å¦‚æœåŒ¹é…2ä¸ªæˆ–ä»¥ä¸Šå…³é”®è¯ï¼Œåˆ™è®¤ä¸ºé«˜åº¦ç›¸å…³
    return matched_keywords >= 1

def fetch_ai_news():
    """è·å–æ‰€æœ‰AIç›¸å…³çš„æ–°é—»"""
    ai_stories = []
    
    print("ğŸ” æ­£åœ¨è·å–æœ€æ–°æ•…äº‹...")
    story_ids = get_top_stories()
    
    for idx, story_id in enumerate(story_ids):
        item = get_item(story_id)
        if not item:
            continue
            
        # è·³è¿‡å·²åˆ é™¤çš„é¡¹ç›®
        if item.get('deleted') or item.get('dead'):
            continue
            
        title = item.get('title', '')
        text = item.get('text', '')
        
        # æ£€æŸ¥æ˜¯å¦ä¸AIç›¸å…³
        if is_ai_related(title, text):
            ai_stories.append({
                'id': item['id'],
                'title': title,
                'url': item.get('url', ''),
                'score': item.get('score', 0),
                'by': item.get('by', 'Unknown'),
                'time': item.get('time', 0),
                'comments': item.get('descendants', 0),
                'hn_url': f"https://news.ycombinator.com/item?id={item['id']}"
            })
        
        # æ˜¾ç¤ºè¿›åº¦
        if (idx + 1) % 10 == 0:
            print(f"  å·²æ£€æŸ¥ {idx + 1}/{len(story_ids)} ä¸ªæ•…äº‹...")
    
    return ai_stories

def format_results(ai_stories):
    """æ ¼å¼åŒ–ç»“æœä¸ºMarkdown"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    content = f"""# ğŸ¤– Hacker News AIç›¸å…³å†…å®¹ç­›é€‰ç»“æœ

**æ›´æ–°æ—¶é—´**: {timestamp}  
**æ‰¾åˆ°**: {len(ai_stories)} ç¯‡AIç›¸å…³å†…å®¹

---

"""
    
    # æŒ‰åˆ†æ•°æ’åº
    ai_stories.sort(key=lambda x: x['score'], reverse=True)
    
    for idx, story in enumerate(ai_stories, 1):
        content += f"""## {idx}. {story['title']}

- **æ¥æº**: [Hacker News](https://news.ycombinator.com/item?id={story['id']})
- **åˆ†æ•°**: {story['score']} ğŸ‘
- **è¯„è®º**: {story['comments']} ğŸ’¬
- **ä½œè€…**: {story['by']}
- **é“¾æ¥**: [{story['url'][:50]}...]({story['url']}) (å¦‚æœæœ‰)

---

"""
    
    return content

def main():
    print("ğŸš€ å¼€å§‹è·å–Hacker News AIç›¸å…³å†…å®¹...")
    
    # è·å–AIç›¸å…³çš„æ–°é—»
    ai_stories = fetch_ai_news()
    
    print(f"\\nâœ… æ‰¾åˆ° {len(ai_stories)} ç¯‡AIç›¸å…³å†…å®¹")
    
    # æ ¼å¼åŒ–ä¸ºMarkdown
    markdown_content = format_results(ai_stories)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open('AI_NEWS.md', 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    print("ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° AI_NEWS.md")
    
    # åŒæ—¶ä¿å­˜JSONæ ¼å¼ï¼ˆç”¨äºå…¶ä»–å¤„ç†ï¼‰
    with open('ai_news.json', 'w', encoding='utf-8') as f:
        json.dump(ai_stories, f, ensure_ascii=False, indent=2)
    
    print("ğŸ“Š JSONæ•°æ®å·²ä¿å­˜åˆ° ai_news.json")

if __name__ == "__main__":
    main()
